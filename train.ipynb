{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a55eec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2979\n",
      "2999\n"
     ]
    }
   ],
   "source": [
    "%store -r n_user\n",
    "%store -r n_item\n",
    "\n",
    "print(n_user)\n",
    "print(n_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd76bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "execution role ARN: arn:aws:iam::629515838455:role/SageMaker_smwu\n",
      "sagemaker-ap-northeast-2-629515838455\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "import os\n",
    "import json\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# get current SageMaker session's execution role and default bucket name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"execution role ARN:\", role)\n",
    "\n",
    "# bucket_name_default = sagemaker_session.default_bucket()\n",
    "# print(bucket_name_default)\n",
    "# bucket_name = 'ml-demo-common'\n",
    "%store -r bucket_name\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd5da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-629515838455/data\n"
     ]
    }
   ],
   "source": [
    "# specify the location of the training data\n",
    "training_data_uri = os.path.join(f's3://{bucket_name}', 'data')\n",
    "print(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3be8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\n",
      "\u001b[33m SPDX-License-Identifier: MIT-0\u001b[39;49;00m\n",
      "\u001b[33m \u001b[39;49;00m\n",
      "\u001b[33m Permission is hereby granted, free of charge, to any person obtaining a copy of this\u001b[39;49;00m\n",
      "\u001b[33m software and associated documentation files (the \"Software\"), to deal in the Software\u001b[39;49;00m\n",
      "\u001b[33m without restriction, including without limitation the rights to use, copy, modify,\u001b[39;49;00m\n",
      "\u001b[33m merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\u001b[39;49;00m\n",
      "\u001b[33m permit persons to whom the Software is furnished to do so.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\u001b[39;49;00m\n",
      "\u001b[33m INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\u001b[39;49;00m\n",
      "\u001b[33m PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\u001b[39;49;00m\n",
      "\u001b[33m HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\u001b[39;49;00m\n",
      "\u001b[33m OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\u001b[39;49;00m\n",
      "\u001b[33m SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# for data processing\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(base_dir):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" load training data \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    df_train = np.load(os.path.join(base_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    user_train, item_train, y_train = np.split(np.transpose(df_train).flatten(), \u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_train, item_train, y_train\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbatch_generator\u001b[39;49;00m(x, y, batch_size, n_batch, shuffle, user_dim, item_dim):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" batch generator to supply data for training and testing \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    user_df, item_df = x\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    counter = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    training_index = np.arange(user_df.shape[\u001b[34m0\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m shuffle:\u001b[37m\u001b[39;49;00m\n",
      "        np.random.shuffle(training_index)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m \u001b[34mTrue\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        batch_index = training_index[batch_size*counter:batch_size*(counter+\u001b[34m1\u001b[39;49;00m)]\u001b[37m\u001b[39;49;00m\n",
      "        user_batch = tf.one_hot(user_df[batch_index], depth=user_dim)\u001b[37m\u001b[39;49;00m\n",
      "        item_batch = tf.one_hot(item_df[batch_index], depth=item_dim)\u001b[37m\u001b[39;49;00m\n",
      "        y_batch = y[batch_index]\u001b[37m\u001b[39;49;00m\n",
      "        counter += \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34myield\u001b[39;49;00m [user_batch, item_batch], y_batch\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m counter == n_batch:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m shuffle:\u001b[37m\u001b[39;49;00m\n",
      "                np.random.shuffle(training_index)\u001b[37m\u001b[39;49;00m\n",
      "            counter = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# network\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_user_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" create user embeddings \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    user_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    user_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m user_gmf_emb, user_mlp_emb\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_item_embedding_layers\u001b[39;49;00m(inputs, emb_dim):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" create item embeddings \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    item_gmf_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    item_mlp_emb = tf.keras.layers.Dense(emb_dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m item_gmf_emb, item_mlp_emb\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_gmf\u001b[39;49;00m(user_emb, item_emb):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" general matrix factorization branch \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    gmf_mat = tf.keras.layers.Multiply()([user_emb, item_emb])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m gmf_mat\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_mlp\u001b[39;49;00m(user_emb, item_emb, dropout_rate):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" multi-layer perceptron branch \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32madd_layer\u001b[39;49;00m(dim, input_layer, dropout_rate):\u001b[37m\u001b[39;49;00m\n",
      "        hidden_layer = tf.keras.layers.Dense(dim, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(input_layer)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m dropout_rate:\u001b[37m\u001b[39;49;00m\n",
      "            dropout_layer = tf.keras.layers.Dropout(dropout_rate)(hidden_layer)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mreturn\u001b[39;49;00m dropout_layer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m hidden_layer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    concat_layer = tf.keras.layers.Concatenate()([user_emb, item_emb])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dropout_l1 = tf.keras.layers.Dropout(dropout_rate)(concat_layer)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dense_layer_1 = add_layer(\u001b[34m64\u001b[39;49;00m, dropout_l1, dropout_rate)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dense_layer_2 = add_layer(\u001b[34m32\u001b[39;49;00m, dense_layer_1, dropout_rate)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dense_layer_3 = add_layer(\u001b[34m16\u001b[39;49;00m, dense_layer_2, \u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dense_layer_4 = add_layer(\u001b[34m8\u001b[39;49;00m, dense_layer_3, \u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m dense_layer_4\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_neuCF\u001b[39;49;00m(gmf, mlp, dropout_rate):\u001b[37m\u001b[39;49;00m\n",
      "    concat_layer = tf.keras.layers.Concatenate()([gmf, mlp])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    output_layer = tf.keras.layers.Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(concat_layer)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m output_layer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_graph\u001b[39;49;00m(user_dim, item_dim, dropout_rate=\u001b[34m0.25\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m    \u001b[39;49;00m\u001b[33m\"\"\" neural collaborative filtering model \"\"\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    user_input = tf.keras.Input(shape=(user_dim))\u001b[37m\u001b[39;49;00m\n",
      "    item_input = tf.keras.Input(shape=(item_dim))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# create embedding layers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    user_gmf_emb, user_mlp_emb = _get_user_embedding_layers(user_input, \u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    item_gmf_emb, item_mlp_emb = _get_item_embedding_layers(item_input, \u001b[34m32\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# general matrix factorization\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    gmf = _gmf(user_gmf_emb, item_gmf_emb)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# multi layer perceptron\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    mlp = _mlp(user_mlp_emb, item_mlp_emb, dropout_rate)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# output\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    output = _neuCF(gmf, mlp, dropout_rate)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# create the model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = tf.keras.Model(inputs=[user_input, item_input], outputs=output)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel\u001b[39;49;00m(x_train, y_train, n_user, n_item, num_epoch, batch_size):\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    num_batch = np.ceil(x_train[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]/batch_size)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# build graph\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model = build_graph(n_user, n_item)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# compile and train\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=\u001b[34m1e-3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model.compile(optimizer=optimizer,\u001b[37m\u001b[39;49;00m\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\u001b[37m\u001b[39;49;00m\n",
      "                  metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model.fit_generator(\u001b[37m\u001b[39;49;00m\n",
      "        generator=batch_generator(\u001b[37m\u001b[39;49;00m\n",
      "            x=x_train, y=y_train,\u001b[37m\u001b[39;49;00m\n",
      "            batch_size=batch_size, n_batch=num_batch,\u001b[37m\u001b[39;49;00m\n",
      "            shuffle=\u001b[34mTrue\u001b[39;49;00m, user_dim=n_user, item_dim=n_item),\u001b[37m\u001b[39;49;00m\n",
      "        epochs=num_epoch,\u001b[37m\u001b[39;49;00m\n",
      "        steps_per_epoch=num_batch,\u001b[37m\u001b[39;49;00m\n",
      "        verbose=\u001b[34m2\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_user\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_item\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    args, unknown = _parse_args()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    user_train, item_train, train_labels = _load_training_data(args.train)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# build model\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    ncf_model = model(\u001b[37m\u001b[39;49;00m\n",
      "        x_train=[user_train, item_train],\u001b[37m\u001b[39;49;00m\n",
      "        y_train=train_labels,\u001b[37m\u001b[39;49;00m\n",
      "        n_user=args.n_user,\u001b[37m\u001b[39;49;00m\n",
      "        n_item=args.n_item,\u001b[37m\u001b[39;49;00m\n",
      "        num_epoch=args.epochs,\u001b[37m\u001b[39;49;00m\n",
      "        batch_size=args.batch_size\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# save model to an S3 directory with version number '00000001'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        ncf_model.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m000000001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mneural_collaborative_filtering.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# inspect the training script using `pygmentize` magic\n",
    "!pygmentize 'ncf.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5e893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training instance type and model hyperparameters\n",
    "# note that for the demo purpose, the number of epoch is set to 1\n",
    "\n",
    "num_of_instance = 1                 # number of instance to use for training\n",
    "instance_type = 'ml.m5.large'     # type of instance to use for training\n",
    "\n",
    "training_script = 'ncf.py'\n",
    "\n",
    "training_parameters = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 256, \n",
    "    'n_user': n_user, \n",
    "    'n_item': n_item\n",
    "}\n",
    "\n",
    "# training framework specs\n",
    "tensorflow_version = '2.1.0'\n",
    "python_version = 'py3'\n",
    "distributed_training_spec = {'parameter_server': {'enabled': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf7239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# initiate the training job using Tensorflow estimator\n",
    "ncf_estimator = TensorFlow(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    instance_count=num_of_instance,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    distribution=distributed_training_spec,\n",
    "    hyperparameters=training_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6866f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Amazon SageMaker Debugger does not currently support Parameter Server distribution\n",
      "INFO:sagemaker:Amazon SageMaker Debugger does not currently support Parameter Server distribution\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2023-12-05-05-14-52-945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-12-05 05:14:53 Starting - Starting the training job...\n",
      "2023-12-05 05:15:10 Starting - Preparing the instances for training......\n",
      "2023-12-05 05:16:15 Downloading - Downloading input data.........\n",
      "2023-12-05 05:17:41 Training - Training image download completed. Training in progress.\u001b[34m2023-12-05 05:17:44,330 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2023-12-05 05:17:44,338 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 05:17:44,556 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 05:17:44,573 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 05:17:44,588 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-12-05 05:17:44,599 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 256,\n",
      "        \"epochs\": 1,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model\",\n",
      "        \"n_item\": 2999,\n",
      "        \"n_user\": 2979\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2023-12-05-05-14-52-945\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ncf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ncf.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model\",\"n_item\":2999,\"n_user\":2979}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ncf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ncf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":256,\"epochs\":1,\"model_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model\",\"n_item\":2999,\"n_user\":2979},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2023-12-05-05-14-52-945\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/source/sourcedir.tar.gz\",\"module_name\":\"ncf\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ncf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"256\",\"--epochs\",\"1\",\"--model_dir\",\"s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model\",\"--n_item\",\"2999\",\"--n_user\",\"2979\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model\u001b[0m\n",
      "\u001b[34mSM_HP_N_ITEM=2999\u001b[0m\n",
      "\u001b[34mSM_HP_N_USER=2979\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 ncf.py --batch_size 256 --epochs 1 --model_dir s3://sagemaker-ap-northeast-2-629515838455/tensorflow-training-2023-12-05-05-14-52-945/model --n_item 2999 --n_user 2979\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From ncf.py:168: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mPlease use Model.fit, which supports generators.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\u001b[0m\n",
      "\u001b[34mTrain for 1894.0 steps\u001b[0m\n",
      "\u001b[34m1894/1894 - 30s - loss: 0.4609 - accuracy: 0.8310\u001b[0m\n",
      "\u001b[34m2023-12-05 05:18:17.442524: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\u001b[0m\n",
      "\u001b[34m2023-12-05 05:18:17.948459: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/000000001/assets\u001b[0m\n",
      "\u001b[34m2023-12-05 05:18:18,798 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-12-05 05:18:38 Uploading - Uploading generated training model\n",
      "2023-12-05 05:18:38 Completed - Training job completed\n",
      "Training seconds: 142\n",
      "Billable seconds: 142\n"
     ]
    }
   ],
   "source": [
    "# kick off the training job\n",
    "ncf_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c97a87a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating model with name: rest-recommend-model-sg02\n",
      "WARNING:sagemaker:Using already existing model: rest-recommend-model-sg02\n",
      "INFO:sagemaker:Creating endpoint-config with name ep-rest-rec-002\n",
      "INFO:sagemaker:Creating endpoint with name ep-rest-rec-002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'ep-rest-rec-002'\n",
    "model_name = 'rest-recommend-model-sg02'\n",
    "\n",
    "predictor = ncf_estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m5.large\", \n",
    "    endpoint_name=endpoint_name,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf304ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name= \"ep-rest-rec-002\"\n",
      "bucket_name = \"sagemaker-ap-northeast-2-629515838455\"\n"
     ]
    }
   ],
   "source": [
    "print(f'endpoint_name= \"{endpoint_name}\"')\n",
    "print(f'bucket_name = \"{bucket_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f86fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254481fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf9ec65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: ep-rest-rec-sg02\n",
      "INFO:sagemaker:Deleting endpoint with name: ep-rest-rec-sg02\n"
     ]
    }
   ],
   "source": [
    "# delete endpoint at the end of the demo\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1858c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
